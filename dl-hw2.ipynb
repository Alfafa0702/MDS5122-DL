{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","cell_execution_strategy":"setup"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11503849,"sourceType":"datasetVersion","datasetId":7212539}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport re\nimport random\nimport json\n\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"8rmBu5x_F914","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:23.535192Z","iopub.execute_input":"2025-04-28T08:52:23.535460Z","iopub.status.idle":"2025-04-28T08:52:28.287656Z","shell.execute_reply.started":"2025-04-28T08:52:23.535439Z","shell.execute_reply":"2025-04-28T08:52:28.287069Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\nSOS_token = tokenizer.bos_token_id  # 使用 GPT2 的 <bos> 作为 SOS\nEOS_token = tokenizer.eos_token_id  # 使用 GPT2 的 <eos> 作为 EOS\nvocab_size = len(tokenizer)\n\n# For English, lowercase, trim, and remove non-letter characters\ndef normalizeEng(s):\n    '''\n    process the English sentence\n    '''\n    s = s.lower().strip() # remove leading and trailing spaces then tokenize\n    s = re.sub(r\"([.!?])\", r\" \\1\", s) # add spaces around .!? to separate them from words\n    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s) # replace other characters with spaces\n    return s.strip()\n# For Chinese, trim, and remove other characters\ndef normalizeChi(s):\n    s = s.strip()\n    s = re.sub(r\"([.!?。])\", r\" \\1 \", s)\n    s = re.sub(r\"[^\\u4e00-\\u9fff\\sa-zA-Z!！?。.]+\", r\" \", s)\n    return s.strip()","metadata":{"id":"TMYe8T7uGRqM","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:28.288827Z","iopub.execute_input":"2025-04-28T08:52:28.289504Z","iopub.status.idle":"2025-04-28T08:52:34.467235Z","shell.execute_reply.started":"2025-04-28T08:52:28.289483Z","shell.execute_reply":"2025-04-28T08:52:34.466653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c68e21f3bc84ad6a3a005328aaf4cec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6da04b99b82e44879dbbf48fb5568fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea1047475d3f4b1bbb8e75a7b10a0ca8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a95bec44a6f04720afc891680f74bf30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"debb4f3d640948498013367cb4d17e67"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def readLangs():\n    '''\n    read txt file, process sentence, convert them into pairs\n    '''\n    print(\"Reading lines...\")\n\n    # Read the file and split into lines\n    lines = open(dir, encoding='utf-8').read().strip().split('\\n')\n\n    # Split every line into chinese english pairs and normalize\n    pairs = [[normalizeChi(l.split('\\t')[1]),normalizeEng(l.split('\\t')[0])] for l in lines]\n\n    return pairs","metadata":{"id":"8aHM4Dd-GRsZ","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.467886Z","iopub.execute_input":"2025-04-28T08:52:34.468253Z","iopub.status.idle":"2025-04-28T08:52:34.472801Z","shell.execute_reply.started":"2025-04-28T08:52:34.468234Z","shell.execute_reply":"2025-04-28T08:52:34.472180Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def indexesFromSentence(sentence):\n    return tokenizer(sentence)[\"input_ids\"]\n\ndef tensorFromSentence(sentence):\n    indexes = indexesFromSentence(sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(pair[0])\n    target_tensor = tensorFromSentence(pair[1])\n    return (input_tensor, target_tensor)\n\nfrom torch.utils.data import random_split\ntorch.manual_seed(42)\ndef get_dataloader(batch_size, MAX_LENGTH=100):\n    pairs = readLangs()\n\n    n = len(pairs)\n    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n\n    for idx, (inp, tgt) in enumerate(pairs):\n        inp_ids = indexesFromSentence(inp)\n        tgt_ids = indexesFromSentence(tgt)\n        inp_ids.append(EOS_token)\n        tgt_ids.append(EOS_token)\n        input_ids[idx, :len(inp_ids)] = inp_ids\n        target_ids[idx, :len(tgt_ids)] = tgt_ids\n\n    data = TensorDataset(torch.LongTensor(input_ids).to(device),\n                               torch.LongTensor(target_ids).to(device))\n\n    train_size = int(0.9 * len(data))\n    test_size = len(data) - train_size\n\n    train_dataset, test_dataset = random_split(data, [train_size, test_size])\n\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    return train_dataloader, test_dataloader","metadata":{"id":"GqCGhMvbGRuI","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.474416Z","iopub.execute_input":"2025-04-28T08:52:34.474613Z","iopub.status.idle":"2025-04-28T08:52:34.493651Z","shell.execute_reply.started":"2025-04-28T08:52:34.474593Z","shell.execute_reply":"2025-04-28T08:52:34.492949Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import time\n\ndef train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n                decoder_optimizer, criterion):\n    total_loss = 0\n    start_time = time.time()\n    s_time = time.time()\n\n    for i, data in enumerate(dataloader):\n        input_tensor, target_tensor = data\n\n        encoder_optimizer.zero_grad()\n        decoder_optimizer.zero_grad()\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n\n        loss = criterion(\n            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n            target_tensor.view(-1)\n        )\n        loss.backward()\n\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n        total_loss += loss.item()\n\n        if i % 100 == 0:\n            print(f'Within one epoch: training {i}/{len(dataloader)} | Time: {time.time() - s_time:.2f}s')\n            s_time = time.time()\n\n    end_time = time.time()\n    epoch_time = end_time - start_time\n    print(f'\\nOne Epoch finished | Total time: {epoch_time:.2f}s | Average loss: {total_loss / len(dataloader):.4f}')\n\n    return total_loss / len(dataloader)","metadata":{"id":"47Pg-jV7GRxn","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.494391Z","iopub.execute_input":"2025-04-28T08:52:34.494657Z","iopub.status.idle":"2025-04-28T08:52:34.513411Z","shell.execute_reply.started":"2025-04-28T08:52:34.494636Z","shell.execute_reply":"2025-04-28T08:52:34.512831Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Encoder","metadata":{"id":"haxLziwjQB6h"}},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, input):\n        embedded = self.dropout(self.embedding(input))\n        output, hidden = self.gru(embedded)\n        return output, hidden","metadata":{"id":"3Db62jgGGr7N","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.514021Z","iopub.execute_input":"2025-04-28T08:52:34.514260Z","iopub.status.idle":"2025-04-28T08:52:34.531846Z","shell.execute_reply.started":"2025-04-28T08:52:34.514234Z","shell.execute_reply":"2025-04-28T08:52:34.531305Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Decoder with attention","metadata":{"id":"hr0iXpy3QQcQ"}},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, hidden_size):\n        super(BahdanauAttention, self).__init__()\n        self.Wa = nn.Linear(hidden_size, hidden_size)\n        self.Ua = nn.Linear(hidden_size, hidden_size)\n        self.Va = nn.Linear(hidden_size, 1)\n\n    def forward(self, query, keys):\n        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n        scores = scores.squeeze(2).unsqueeze(1)\n\n        weights = F.softmax(scores, dim=-1)\n        context = torch.bmm(weights, keys)\n\n        return context, weights\n\nclass AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n        super(AttnDecoderRNN, self).__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.attention = BahdanauAttention(hidden_size)\n        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n        batch_size = encoder_outputs.size(0)\n        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n        decoder_hidden = encoder_hidden\n        decoder_outputs = []\n        attentions = []\n\n        for i in range(MAX_LENGTH):\n            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n                decoder_input, decoder_hidden, encoder_outputs\n            )\n            decoder_outputs.append(decoder_output)\n            attentions.append(attn_weights)\n\n            if target_tensor is not None:\n                # Teacher forcing: Feed the target as the next input\n                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n            else:\n                # Without teacher forcing: use its own predictions as the next input\n                _, topi = decoder_output.topk(1)\n                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n\n        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n        attentions = torch.cat(attentions, dim=1)\n\n        return decoder_outputs, decoder_hidden, attentions\n\n\n    def forward_step(self, input, hidden, encoder_outputs):\n        embedded =  self.dropout(self.embedding(input))\n\n        query = hidden.permute(1, 0, 2)\n        context, attn_weights = self.attention(query, encoder_outputs)\n        input_gru = torch.cat((embedded, context), dim=2)\n\n        output, hidden = self.gru(input_gru, hidden)\n        output = self.out(output)\n\n        return output, hidden, attn_weights","metadata":{"id":"R0TXVum1QYfH","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.532605Z","iopub.execute_input":"2025-04-28T08:52:34.532852Z","iopub.status.idle":"2025-04-28T08:52:34.549284Z","shell.execute_reply.started":"2025-04-28T08:52:34.532830Z","shell.execute_reply":"2025-04-28T08:52:34.548505Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Training","metadata":{"id":"eqGMQGQBQq8Y"}},{"cell_type":"code","source":"import time\nimport math\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"id":"L5CRElCYQk8x","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.550126Z","iopub.execute_input":"2025-04-28T08:52:34.550348Z","iopub.status.idle":"2025-04-28T08:52:34.567515Z","shell.execute_reply.started":"2025-04-28T08:52:34.550324Z","shell.execute_reply":"2025-04-28T08:52:34.567058Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n\ndef showPlot(plot_losses, save_plot_path=\"training_loss.png\"):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # 设置y轴的刻度间隔\n    loc = ticker.MultipleLocator(base=0.01)\n    ax.yaxis.set_major_locator(loc)\n    # 设置y轴的刻度格式，保留两位小数\n    formatter = ticker.FormatStrFormatter('%.2f')\n    ax.yaxis.set_major_formatter(formatter)\n    plt.plot(plot_losses.keys(), plot_losses.values())\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Training Loss\")\n    plt.savefig(save_plot_path)  # 保存图像\n    plt.show()","metadata":{"id":"XhhyuouoQlFM","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.568257Z","iopub.execute_input":"2025-04-28T08:52:34.568514Z","iopub.status.idle":"2025-04-28T08:52:34.583612Z","shell.execute_reply.started":"2025-04-28T08:52:34.568489Z","shell.execute_reply":"2025-04-28T08:52:34.582893Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd\nimport pickle\n\ndef train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,save_pkl_path=\"losses.pkl\"):\n    start = time.time()\n    losses = {}\n\n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    criterion = nn.NLLLoss()\n\n    for epoch in tqdm(range(1, n_epochs + 1),desc=\"Training Epochs...\"):\n        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n        losses[epoch] = loss\n        print('%s (%d %d%%) loss: %.4f' % (timeSince(start, epoch / n_epochs),\n                                        epoch, epoch / n_epochs * 100, loss))\n        print(f'==== Finished training of {epoch}/{n_epochs} epochs，loss is {loss} ====')\n\n    showPlot(losses)\n    with open(save_pkl_path, 'wb') as f:\n        pickle.dump(losses, f)","metadata":{"id":"68H-g_i6QlCD","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.585771Z","iopub.execute_input":"2025-04-28T08:52:34.586127Z","iopub.status.idle":"2025-04-28T08:52:34.966840Z","shell.execute_reply.started":"2025-04-28T08:52:34.586111Z","shell.execute_reply":"2025-04-28T08:52:34.966332Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Evaluation & attention visualization","metadata":{"id":"ZoVcJDayQze0"}},{"cell_type":"code","source":"def tensor2sentence(input_tensor):\n    return tokenizer.decode(input_tensor)\ndef evaluate(encoder, decoder, input_tensor):\n    with torch.no_grad():\n        encoder_outputs, encoder_hidden = encoder(input_tensor)\n        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n\n        _, topi = decoder_outputs.topk(1)\n        decoded_ids = topi.squeeze()[0]\n\n        decoded_sentence = tensor2sentence(decoded_ids)\n    return decoded_sentence, decoder_attn","metadata":{"id":"Ab1mlwC-QlIp","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:34.967452Z","iopub.execute_input":"2025-04-28T08:52:34.967733Z","iopub.status.idle":"2025-04-28T08:52:34.972182Z","shell.execute_reply.started":"2025-04-28T08:52:34.967717Z","shell.execute_reply":"2025-04-28T08:52:34.971395Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# visualize attention\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nmyfont = fm.FontProperties(fname=\"/kaggle/input/simhei/SIMHEI.TTF\") #Kaggle需要使用上传的黑体\n\ndef showAttention(input_sentence, output_words, attentions):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.cpu().numpy(), cmap='plasma')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + list(input_sentence) +\n                       ['<EOS>'], rotation=90,fontproperties=myfont)\n    ax.set_yticklabels([''] + output_words.split(' ')+\n                       ['<EOS>'])\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.savefig(f'attention_map_{input_sentence}.png')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:17:39.849183Z","iopub.execute_input":"2025-04-28T09:17:39.849871Z","iopub.status.idle":"2025-04-28T09:17:39.855288Z","shell.execute_reply.started":"2025-04-28T09:17:39.849849Z","shell.execute_reply":"2025-04-28T09:17:39.854560Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"import json\ndef evaluateRandomly(dataloader, encoder, decoder, save_path='evaluation.json'):\n    results = []  # 创建一个空列表来存储结果\n    for data in dataloader:\n        input_tensor, target_tensor = data\n        input_sentence = tensor2sentence(input_tensor[0]).split('<|endoftext|>', 1)[0]\n        target_sentence = tensor2sentence(target_tensor[0]).split('<|endoftext|', 1)[0]\n        output_sentence, decoder_attn = evaluate(encoder, decoder, input_tensor)\n        output_sentence = output_sentence.split('<|endoftext|', 1)[0]\n\n        # 打印结果\n        print('>', input_sentence)\n        print('=', target_sentence)\n        print('<', output_sentence)\n        print('')\n        \n        showAttention(input_sentence, output_sentence, decoder_attn[0, :len(output_sentence.split(' '))+1, :len(list(input_sentence))+1])\n        \n        # 将结果添加到列表中\n        results.append({\n            'input': input_sentence,\n            'target': target_sentence,\n            'output': output_sentence.split('<|endoftext|>', 1)[0]\n        })\n\n    # 将结果列表写入JSON文件\n    with open(save_path, 'w', encoding='utf-8') as f:\n        json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"id":"JGHvlDl4RNcx","trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:28:04.729390Z","iopub.execute_input":"2025-04-28T09:28:04.729926Z","iopub.status.idle":"2025-04-28T09:28:04.736394Z","shell.execute_reply.started":"2025-04-28T09:28:04.729904Z","shell.execute_reply":"2025-04-28T09:28:04.735816Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"## main","metadata":{"id":"V44piJWXSfrR"}},{"cell_type":"code","source":"hidden_size = 128\nbatch_size = 32\ndir = '/kaggle/input/cmn-eng/cmn.txt'\nMAX_LENGTH = 100\n\ntrain_dataloader, test_dataloader = get_dataloader(batch_size)\n\nencoder = EncoderRNN(vocab_size, hidden_size).to(device)\ndecoder = AttnDecoderRNN(hidden_size, vocab_size).to(device)\n\ntrain(train_dataloader, encoder, decoder, 80)","metadata":{"id":"uXXyeWZlSgwP","outputId":"8279b7cc-0c66-4c3b-fb3b-354be519581c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 保存模型\nencoder_path = 'encoder_model.pth'\ndecoder_path = 'decoder_model.pth'\n\ntorch.save(encoder.state_dict(), encoder_path)\ntorch.save(decoder.state_dict(), decoder_path)\n\nprint(f\"Encoder model saved to {encoder_path}\")\nprint(f\"Decoder model saved to {decoder_path}\")","metadata":{"id":"weA68_R5NWbO"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# evaluation\nencoder.eval()\ndecoder.eval()\nevaluateRandomly(test_dataloader,encoder, decoder)","metadata":{"id":"021RkR1NjnTR","outputId":"43aa2f7c-47fa-489c-f27b-3b867cd2ddb2"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load and eval","metadata":{}},{"cell_type":"code","source":"# 创建模型实例\nhidden_size = 128\nbatch_size = 32\ndir = '/kaggle/input/cmn-eng/cmn.txt'\nMAX_LENGTH = 100\n\ntrain_dataloader, test_dataloader = get_dataloader(batch_size)\n\nencoder = EncoderRNN(vocab_size, hidden_size).to(device)\ndecoder = AttnDecoderRNN(hidden_size, vocab_size).to(device)\n\n# 加载模型参数\nencoder_path = '/kaggle/input/rnn-attention-80/encoder_model.pth'\ndecoder_path = '/kaggle/input/rnn-attention-80/decoder_model.pth'\n\n# 加载参数到模型\nencoder.load_state_dict(torch.load(encoder_path))\ndecoder.load_state_dict(torch.load(decoder_path))\n\n# 将模型设置为评估模式\nencoder.eval()\ndecoder.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T08:52:51.582242Z","iopub.execute_input":"2025-04-28T08:52:51.582723Z","iopub.status.idle":"2025-04-28T08:53:03.105210Z","shell.execute_reply.started":"2025-04-28T08:52:51.582701Z","shell.execute_reply":"2025-04-28T08:53:03.104575Z"}},"outputs":[{"name":"stdout","text":"Reading lines...\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"AttnDecoderRNN(\n  (embedding): Embedding(50257, 128)\n  (attention): BahdanauAttention(\n    (Wa): Linear(in_features=128, out_features=128, bias=True)\n    (Ua): Linear(in_features=128, out_features=128, bias=True)\n    (Va): Linear(in_features=128, out_features=1, bias=True)\n  )\n  (gru): GRU(256, 128, batch_first=True)\n  (out): Linear(in_features=128, out_features=50257, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"evaluateRandomly(test_dataloader,encoder, decoder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T09:28:09.124215Z","iopub.execute_input":"2025-04-28T09:28:09.124476Z","iopub.status.idle":"2025-04-28T09:28:38.374692Z","shell.execute_reply.started":"2025-04-28T09:28:09.124457Z","shell.execute_reply":"2025-04-28T09:28:38.374090Z"}},"outputs":[{"name":"stdout","text":"> 你不該那樣跟湯姆講話 。\n= you shouldn t talk to tom that way\n< you shouldn t talk to tom that way\n\n> 书桌上有一把剪刀 。\n= there is a pair of scissors on the desk\n< there is a pair of scissors on the desk\n\n> 她企圖自殺 。\n= she attempted suicide\n< she tried to kill herself\n\n> 我什么都不喜欢 。\n= i don t like anything\n< i don t like anything\n\n> 您想吃点什么吗\n= would you like to eat something ?\n< would you like something to eat ?\n\n> 把這本書放在架子的底部 。\n= put the book on the bottom shelf\n< put the book on the bottom shelf\n\n> 他比我高两寸 。\n= he is two inches taller than i am\n< he is my younger than me\n\n> 他是世界上最有錢的人 。\n= he is the richest man on earth\n< he is the richest man on earth\n\n> 她叫什麼名字\n= what s her name ?\n< what s her name ?\n\n> 你可能該動一動 。\n= maybe you should move\n< you may have been attention\n\n> 我約會遲到了 。\n= i was late for the appointment\n< i will be late\n\n> 年轻人应该尊敬老人 。\n= the young should respect the old\n< the young should respect the old\n\n> 煤和天然氣是天然燃料 。\n= coal and natural gas are natural fuels\n< coal and natural gas are natural fuels\n\n> 我不認為你該改變 。\n= i don t think you need to change\n< i don t think you should open the word\n\n> 喝點冷飲怎樣\n= how about something cold to drink ?\n< how about some cold ?\n\n> 你的回答是错的 。\n= your answer is wrong\n< your answer is wrong\n\n> 在你的词典查一查这个词 。\n= look up the word in your dictionary\n< look up your promise on your word\n\n> 我欠她 美元 。\n= i owe ten dollars to her\n< i owe ten dollars to her\n\n> 我们早饭的面包刚够吃 。\n= we have barely enough bread for breakfast\n< we had barely enough bread for breakfast\n\n> 我们坐在草地上吧 。\n= let s sit on the grass\n< let s sit on the grass\n\n> 我們準備好了走下一步 。\n= we re ready for the next step\n< we had a rest for a while\n\n> 别再插手 。\n= stop meddling\n< stop meddling\n\n> 我知道湯姆累 。\n= i know that tom is tired\n< i know tom is tired\n\n> 這艘船掛著美國國旗 。\n= the ship was flying the american flag\n< the ship made a beautiful tree\n\n> 等候室里有五位患者 。\n= there are five patients in the waiting room\n< there are five patients in the waiting room\n\n> 这辆车靠天然气驱动 。\n= this car runs on natural gas\n< this car runs on natural gas\n\n> 這麼大的夠用嗎\n= is it large enough ?\n< is this jacket safe ?\n\n> 您有几只猫\n= how many cats do you have ?\n< how many cats do you have ?\n\n> 女孩们睡着了\n= the girls were asleep\n< the ladies was on\n\n> 他在桌子底下發現了這個盒子 。\n= he found the box under the table\n< he found the box under the table\n\n> 他给我们讲了一个有趣的故事 我们都笑了起来 。\n= he told us such a funny story that we all laughed\n< we called a funny walk we our eyes and she speaks\n\n> 你可以告诉我我下一步该做什么吗\n= could you tell me what i should do next ?\n< could you tell me what i should do next ?\n\n> 他没有听音乐 。\n= he didn t listen to music\n< he didn t listen to music\n\n> 你想加入哪一組\n= which group do you want to join ?\n< which group do you want to join ?\n\n> 这没有任何意义 。\n= it makes no sense at all\n< it isn t any matter\n\n> 我無法做比這更多的了 。\n= i can t do any more than this\n< i can t do any of these books\n\n> 他應該買一輛二手車的 。\n= he should have bought a used car\n< he should have bought a new car\n\n> 他們把這本小說變成戲劇 。\n= they made the novel into a drama\n< they made the novel into a drama\n\n> 當我讀書的時候 我睡著了 。\n= while i was reading i fell asleep\n< while i was reading i fell asleep\n\n> 這輛公車會載你去博物館 。\n= this bus will take you to the museum\n< this bus will take you to the museum\n\n> 瑪麗把籃子放在桌子上了 。\n= mary set the basket on the table\n< mary set the basket on the table\n\n> 你累嗎   不 一點兒也不 。\n= are you tired ? no not at all\n< you re always kind of direction\n\n> 他試著想讓他妻子開心 但是沒有成功 。\n= he tried to make his wife happy but he couldn t\n< he endeavored to make his wife happy but he couldn t\n\n> 她的左手被燙到了 。\n= she burned her left hand\n< she burned her left hand\n\n> 我是个职员 。\n= i m an office worker\n< i m an office worker\n\n> 那條河流很長 。\n= that river is long\n< that river is dangerous\n\n> 你很聰明 。\n= you re sharp\n< you re smart\n\n> 从什么时候开始您就这里工作着的呢\n= how long have you been working here ?\n< when did you start working here ?\n\n> 你做好了嗎 ?\n= are you done ?\n< are you done ?\n\n> 胃裡如同千軍萬馬開過 。\n= an army marches on its stomach\n< an army marches on its stomach\n\n> 她一直住在小樽 。\n= she has always lived in otaru\n< she is always happy for nothing\n\n> 他唱了些老歌 。\n= he sang some old songs\n< he sang some old songs\n\n> 他們清理了街上的積雪 。\n= they cleared the street of snow\n< they cleared the street of snow\n\n> 我觉得汤姆不太可能会当选 。\n= i think that it s unlikely tom will get elected\n< i think it s unlikely tom will get elected\n\n> 他把梯子倚著柵欄放 。\n= he placed the ladder against the fence\n< he placed the ladder against the fence\n\n> 战争并非不可避免 。\n= war is not inevitable\n< war is not inevitable\n\n> 我不是笨蛋 。\n= i m not stupid\n< i m not stupid\n\n> 我只做別人付錢讓我做的事 。\n= i only do what i m paid to do\n< i only do what i do my future\n\n> 他的声音很悦耳 。\n= he has a pleasant voice\n< he has a pleasant voice\n\n> 他武裝到牙齒 。\n= he is armed to the teeth\n< he is armed to the teeth\n\n> 沒有人生還 。\n= no one escaped alive\n< no one escaped alive\n\n> 他想还清贷款 。\n= he wants to pay off his loan\n< he wants to pay more\n\n> 這個花叫什麼名字\n= what do you call this flower ?\n< what s the nearest name ?\n\n> 我身上帶著很多錢 。\n= i have plenty of money with me\n< i had a lot of money with my shoulder\n\n> 不要把這個錯誤歸咎於她 。\n= don t blame the mistake on her\n< don t blame the mistake on her\n\n> 我發現了真相 。\n= i discovered the truth\n< i discovered the truth\n\n> 看到针管儿之前我一直都很淡定 。\n= i was calm until i saw the syringe\n< i feel like a couple before the day\n\n> 如果一個病人折一千隻紙鶴  她的願望就會成真 。\n= if a sick person folds one thousand paper cranes her wish will come true\n< if a large sum i must have been sure\n\n> 她声称自己没罪 。\n= she declared that she was not guilty\n< she declared that she wasn t guilty\n\n> 这是个很难解决的问题 。\n= this is a difficult problem to solve\n< this is a difficult problem to solve\n\n> 我赢了 。\n= i won !\n< i m wind\n\n> 我们应该禁止针对儿童的广告 。\n= we should ban advertising aimed towards children\n< we should ban advertising aimed towards children\n\n> 现在售票 。\n= tickets are on sale now\n< tickets are on sale now\n\n> 我喜欢跟汤姆唱歌 。\n= i like singing with tom\n< i like to sing with tom\n\n> 如果他们由于什么原因早到的话 请转达他们再等一会儿 。\n= if for some reason they come early please tell them to wait\n< if we went out to say he could wait enough for a while next time\n\n> 大楼看上去那么寒酸 。\n= the buildings look so tiny\n< the buildings look so tiny\n\n> 這是他住的房子 。\n= this is the house where he lives\n< this is where he lives\n\n> 他跑进房间内 。\n= he ran into the room\n< he took the same way for the room\n\n> 這對雙胞胎沒法區分 。\n= the twins are indistinguishable from each other\n< the twins are indistinguishable from each other\n\n> 他在众人面前被嘲笑 。\n= he was laughed at in public\n< he was laughed at in public\n\n> 我喜欢打网球 。\n= i like to play tennis\n< i like playing tennis\n\n> 你的提议值得考虑 。\n= your suggestion is worth considering\n< your suggestion is worth considering\n\n> 这辆车卖得好 。\n= this car sells well\n< this car sells well\n\n> 夜晚很冷 。\n= it was a cold night\n< it s very cold\n\n> 我想要的就是这个 。\n= that s just what i wanted\n< it s exactly what i wanted\n\n> 他喜歡茶 。\n= he likes tea\n< he likes tea\n\n> 有你幫我真好 。\n= it s very kind of you to help me\n< it s good you did you do with me\n\n> 我沒穿內衣 。\n= i m not wearing any underwear\n< i am not wearing any underwear\n\n> 我拥有的书比我能读的书多 。\n= i have more books than i can read\n< i have more books than i can read\n\n> 他送了一張卡片給瑪麗 。\n= he sent a card to mary\n< he sent a card to mary\n\n> 遠離電器設備 。\n= keep away from the electrical equipment\n< keep away from the electrical equipment\n\n> 实话说 我不赞成你 。\n= to tell the truth i don t agree with you\n< frankly speaking i don t agree with you\n\n> 她邊哭邊回答 。\n= she was crying as she answered\n< she cried as she answered\n\n> 最近的藥房在哪裡 ?\n= where s the closest pharmacy ?\n< where is the nearest drugstore ?\n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}