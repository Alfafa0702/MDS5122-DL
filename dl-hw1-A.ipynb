{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Code Example - Part A\n",
    "\n",
    "This code baseline is inspired by and modified from [this great tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "This code can achieve an accuracy of approximately 86.50% on CIFAR-10. Please set up the environment and run your experiments starting from this baseline. You are expected to achieve an accuracy higher than this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-12T11:38:13.830212Z",
     "iopub.status.busy": "2025-03-12T11:38:13.829899Z",
     "iopub.status.idle": "2025-03-12T11:38:19.179381Z",
     "shell.execute_reply": "2025-03-12T11:38:19.178542Z",
     "shell.execute_reply.started": "2025-03-12T11:38:13.830187Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import some necessary packages\n",
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.datasets as tv_datasets\n",
    "import torchvision.transforms as tv_transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:38:19.180673Z",
     "iopub.status.busy": "2025-03-12T11:38:19.180318Z",
     "iopub.status.idle": "2025-03-12T11:38:19.254308Z",
     "shell.execute_reply": "2025-03-12T11:38:19.253703Z",
     "shell.execute_reply.started": "2025-03-12T11:38:19.180649Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "    print(\"CUDA Device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "import torchvision\n",
    "print(f\"torchvision Version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline Model\n",
    "### 1.1 Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 128\n",
    "batch_size = 64\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"Adam\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "# preprocessing pipeline for input images\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    transformation[data_type] = tv_transforms.Compose(([\n",
    "        tv_transforms.RandomRotation(degrees=15),\n",
    "        tv_transforms.RandomHorizontalFlip(),\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    \n",
    "num_workers = 2\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# our network architecture\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network optimizer\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "net.train()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "\n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluating its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = net(img)\n",
    "        \n",
    "        # accumulate\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Variant 1\n",
    "Adding batchnorm dropping the dropout layers in conv blocks, and decreasing the dropout ratio in linear layers.  \n",
    "Time consuming: 50m   \n",
    "last loss: 0.033   \n",
    "Test acc: 89.31%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 128\n",
    "batch_size = 64\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"Adam\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "# preprocessing pipeline for input images\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    transformation[data_type] = tv_transforms.Compose(([\n",
    "        tv_transforms.RandomRotation(degrees=15),\n",
    "        tv_transforms.RandomHorizontalFlip(),\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    \n",
    "num_workers = 2\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# our network architecture\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),nn.BatchNorm2d(128),\n",
    "    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),nn.BatchNorm2d(256),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.3),nn.BatchNorm1d(256),\n",
    "    nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.3),nn.BatchNorm1d(128),\n",
    "    nn.Linear(128, 10),\n",
    ")\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network optimizer\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "losses = []\n",
    "accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # 每个epoch结束后进行一次评估\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "    \n",
    "    # 计算平均损失和准确率\n",
    "    avg_test_loss = test_loss / len(loader[\"test\"])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # 记录结果\n",
    "    losses.append(running_loss / print_every)\n",
    "    accuracies.append(accuracy)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Test Accuracy': accuracies\n",
    "})\n",
    "\n",
    "results_df.to_excel('training_results_Variant1.xlsx', index=False)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 训练损失曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试损失曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试准确率曲线\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_Variant1.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(net.state_dict(), 'Variant1_3.pth')\n",
    "# save model info\n",
    "model_info = {\n",
    "    \"Optimizer\": type(optimizer).__name__,\n",
    "    \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "    \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "    \"Network Architecture\": str(net)\n",
    "}\n",
    "\n",
    "model_info_df = pd.DataFrame([model_info])\n",
    "model_info_df.to_excel('model_info_variant1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluation\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = net(img)\n",
    "        \n",
    "        # accumulate\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variant 2  \n",
    "Add 2 more Conv layers and use wider nets.     \n",
    "last loss: 0.042  \n",
    "Test acc: 89.60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 64\n",
    "batch_size = 64\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"Adam\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "# preprocessing pipeline for input images\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    transformation[data_type] = tv_transforms.Compose(([\n",
    "        tv_transforms.RandomRotation(degrees=15),\n",
    "        tv_transforms.RandomHorizontalFlip(),\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    \n",
    "num_workers = 2\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our network architecture\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n",
    "    nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network optimizer\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "losses = []\n",
    "accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # 每个epoch结束后进行一次评估\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "    \n",
    "    # 计算平均损失和准确率\n",
    "    avg_test_loss = test_loss / len(loader[\"test\"])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # 记录结果\n",
    "    losses.append(running_loss / print_every)\n",
    "    accuracies.append(accuracy)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Test Accuracy': accuracies\n",
    "})\n",
    "\n",
    "results_df.to_excel('training_results_Variant2.xlsx', index=False)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 训练损失曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试损失曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试准确率曲线\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_Variant2.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(net.state_dict(), 'Variant2.pth')\n",
    "# save model info\n",
    "model_info = {\n",
    "    \"Optimizer\": type(optimizer).__name__,\n",
    "    \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "    \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "    \"Network Architecture\": str(net)\n",
    "}\n",
    "\n",
    "model_info_df = pd.DataFrame([model_info])\n",
    "model_info_df.to_excel('model_info_variant2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = net(img)\n",
    "        \n",
    "        # accumulate\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variant 3\n",
    "Adding residual connections to be a ResNet, which allows the network to be deeper while avoiding the problems of vanishing or exploding gradients.  \n",
    "ResNet34:  \n",
    "Time consuming: 1h13m  \n",
    "last loss: 0.129  \n",
    "Test acc: 84.33%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 64\n",
    "batch_size = 64\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"Adam\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "# preprocessing pipeline for input images\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    transformation[data_type] = tv_transforms.Compose(([\n",
    "        tv_transforms.RandomRotation(degrees=15),\n",
    "        tv_transforms.RandomHorizontalFlip(),\n",
    "        tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    ] if is_train else []) + \n",
    "    [\n",
    "        tv_transforms.ToTensor(),\n",
    "        tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "    \n",
    "num_workers = 2\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# ResNet 34\n",
    "# 定义基本的残差块\n",
    "class ResBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# 定义ResNet34模型\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 3, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 3, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * ResBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResBlock(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * ResBlock.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.maxpool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = ResNet34(num_classes=10)\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义基本的残差块\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        # 主路径\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        \n",
    "        # 残差路径\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # 添加残差连接\n",
    "        out = out + self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# 网络结构\n",
    "net = nn.Sequential(\n",
    "    # 第一个卷积块 - 保持与Variant2相同的起始通道数\n",
    "    nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(256),\n",
    "    ResBlock(256, 256),  # 添加残差连接\n",
    "    nn.MaxPool2d(2),\n",
    "    \n",
    "    # 第二个卷积块 - 基于Variant2的通道数\n",
    "    ResBlock(256, 512),\n",
    "    ResBlock(512, 512),\n",
    "    nn.MaxPool2d(2),\n",
    "    \n",
    "    # 第三个卷积块\n",
    "    ResBlock(512, 1024),\n",
    "    ResBlock(1024, 1024),\n",
    "    \n",
    "    # 第四个卷积块\n",
    "    ResBlock(1024, 2048),\n",
    "    ResBlock(2048, 2048),\n",
    "    \n",
    "    # 第五个卷积块\n",
    "    ResBlock(2048, 1024),\n",
    "    nn.MaxPool2d(2),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    # 全连接层 - 与Variant2保持一致\n",
    "    nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n",
    "    nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "\n",
    "# move to device\n",
    "net.to(device)\n",
    "\n",
    "# print the number of parameters\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network optimizer\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 开启异常检测\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 训练循环\n",
    "losses = []\n",
    "accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # 每个epoch结束后进行一次评估\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "    \n",
    "    # 计算平均损失和准确率\n",
    "    avg_test_loss = test_loss / len(loader[\"test\"])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # 记录结果\n",
    "    losses.append(running_loss / print_every)\n",
    "    accuracies.append(accuracy)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Test Accuracy': accuracies\n",
    "})\n",
    "\n",
    "results_df.to_excel('training_results_Variant3.xlsx', index=False)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 训练损失曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试损失曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试准确率曲线\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_Variant3.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(net.state_dict(), 'Variant3.pth')\n",
    "# save model info\n",
    "model_info = {\n",
    "    \"Optimizer\": type(optimizer).__name__,\n",
    "    \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "    \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "    \"Network Architecture\": str(net)\n",
    "}\n",
    "\n",
    "model_info_df = pd.DataFrame([model_info])\n",
    "model_info_df.to_excel('model_info_variant3.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = net(img)\n",
    "        \n",
    "        # accumulate\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variant 4\n",
    "Changing the data augementation.  \n",
    "Ref: https://zhuanlan.zhihu.com/p/49180361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:38:43.674123Z",
     "iopub.status.busy": "2025-03-12T11:38:43.673850Z",
     "iopub.status.idle": "2025-03-12T11:38:50.824034Z",
     "shell.execute_reply": "2025-03-12T11:38:50.823331Z",
     "shell.execute_reply.started": "2025-03-12T11:38:43.674103Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "# preprocessing pipeline for input images\n",
    "# 数据增强\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    if is_train:\n",
    "        # 训练集使用增强的数据增强\n",
    "        transformation[data_type] = tv_transforms.Compose([\n",
    "            # 随机裁剪和填充\n",
    "            tv_transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "            # 随机水平翻转\n",
    "            tv_transforms.RandomHorizontalFlip(),\n",
    "            # 随机旋转\n",
    "            tv_transforms.RandomRotation(degrees=15),\n",
    "            # 随机仿射变换\n",
    "            tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            # 颜色抖动\n",
    "            tv_transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            # 转换为张量\n",
    "            tv_transforms.ToTensor(),\n",
    "            # 标准化\n",
    "            tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            # 随机擦除\n",
    "            tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "        ])\n",
    "    else:\n",
    "        # 测试集只进行基本转换\n",
    "        transformation[data_type] = tv_transforms.Compose([\n",
    "            tv_transforms.ToTensor(),\n",
    "            tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "        \n",
    "num_workers = 2\n",
    "batch_size = 64\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:39:20.117524Z",
     "iopub.status.busy": "2025-03-12T11:39:20.117185Z",
     "iopub.status.idle": "2025-03-12T11:39:20.121664Z",
     "shell.execute_reply": "2025-03-12T11:39:20.120935Z",
     "shell.execute_reply.started": "2025-03-12T11:39:20.117450Z"
    }
   },
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 64\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"AdamW\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-03-12T11:39:26.918169Z",
     "iopub.status.busy": "2025-03-12T11:39:26.917888Z",
     "iopub.status.idle": "2025-03-12T11:39:28.313816Z",
     "shell.execute_reply": "2025-03-12T11:39:28.313084Z",
     "shell.execute_reply.started": "2025-03-12T11:39:26.918149Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 使用Variant2的网络架构\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n",
    "    nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "# 移动到设备\n",
    "net.to(device)\n",
    "\n",
    "# 打印参数数量\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:39:56.493569Z",
     "iopub.status.busy": "2025-03-12T11:39:56.493263Z",
     "iopub.status.idle": "2025-03-12T11:39:56.623213Z",
     "shell.execute_reply": "2025-03-12T11:39:56.621658Z",
     "shell.execute_reply.started": "2025-03-12T11:39:56.493543Z"
    }
   },
   "outputs": [],
   "source": [
    "# the network optimizer\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 开启异常检测\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 训练循环\n",
    "losses = []\n",
    "accuracies = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # 每个epoch结束后进行一次评估\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "    \n",
    "    # 计算平均损失和准确率\n",
    "    avg_test_loss = test_loss / len(loader[\"test\"])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # 记录结果\n",
    "    losses.append(running_loss / print_every)\n",
    "    accuracies.append(accuracy)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Test Accuracy': accuracies\n",
    "})\n",
    "\n",
    "results_df.to_excel('training_results_Variant5.xlsx', index=False)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 训练损失曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试损失曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试准确率曲线\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_Variant5.jpg')\n",
    "plt.show()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(net.state_dict(), 'Variant5.pth')\n",
    "# 保存模型信息\n",
    "model_info = {\n",
    "    \"Optimizer\": type(optimizer).__name__,\n",
    "    \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "    \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "    \"Network Architecture\": str(net),\n",
    "    \"Data Augmentation\": \"Enhanced\"\n",
    "}\n",
    "\n",
    "model_info_df = pd.DataFrame([model_info])\n",
    "model_info_df.to_excel('model_info_variant5.xlsx', index=False)\n",
    "\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = net(img)\n",
    "        \n",
    "        # accumulate\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Variant 5\n",
    "Changing the optimizer and learning rate scheduler.  \n",
    "Add warmup in the first warmup_epochs, then keeping max_lr and cooldown with cosinelr after 50% num_epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare datasets\n",
    "# preprocessing pipeline for input images\n",
    "# 数据增强\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    if is_train:\n",
    "        # 训练集使用增强的数据增强\n",
    "        transformation[data_type] = tv_transforms.Compose([\n",
    "            # 随机裁剪和填充\n",
    "            tv_transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
    "            # 随机水平翻转\n",
    "            tv_transforms.RandomHorizontalFlip(),\n",
    "            # 随机旋转\n",
    "            tv_transforms.RandomRotation(degrees=15),\n",
    "            # 随机仿射变换\n",
    "            tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            # 颜色抖动\n",
    "            tv_transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            # 转换为张量\n",
    "            tv_transforms.ToTensor(),\n",
    "            # 标准化\n",
    "            tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            # 随机擦除\n",
    "            tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "        ])\n",
    "    else:\n",
    "        # 测试集只进行基本转换\n",
    "        transformation[data_type] = tv_transforms.Compose([\n",
    "            tv_transforms.ToTensor(),\n",
    "            tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "        \n",
    "num_workers = 2\n",
    "batch_size = 64\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.CIFAR10(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some experimental setup\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 64\n",
    "print_every = 200\n",
    "\n",
    "optim_name = \"AdamW\"\n",
    "optim_kwargs = dict(\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "AdamW lr=3e-4,weight_decay=0.01 scheduler = lr_scheduler.StepLR(optimizer, step_size=15,gamma=0.1) last_loss = 0.464 acc = 83.82%\n",
    "\n",
    "AdamW lr=3e-4,weight_decay=0.01\n",
    "constant + cooldown last_loss = 1.015 acc = 68.65%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 使用Variant2的网络架构\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n",
    "    nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "# 移动到设备\n",
    "net.to(device)\n",
    "\n",
    "# 打印参数数量\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the network optimizer\n",
    "# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# # loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # the network lr_scheduler\n",
    "# # scheduler = lr_scheduler.StepLR(optimizer, step_size=15,gamma=0.1)\n",
    "# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=num_epochs) #或者20？\n",
    "# # constant + cooldown\n",
    "# def update_learn_rate(optimizer, alpha):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = alpha\n",
    "# # training loop\n",
    "# net.train()\n",
    "# for epoch in range(num_epochs):\n",
    "    \n",
    "#     running_loss = 0.0\n",
    "#     alpha = lr\n",
    "#     for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "#         img, target = img.to(device), target.to(device)\n",
    "#         # 调整学习率\n",
    "#         if epoch <= 30 and (epoch + 1) % 5 == 0:\n",
    "#             alpha *= 0.98\n",
    "#             update_learn_rate(optimizer, alpha)  # 更新学习率\n",
    "#         elif epoch > 30 and epoch <= 70 and (epoch + 1) % 5 == 0:\n",
    "#             alpha *= 0.95\n",
    "#             update_learn_rate(optimizer, alpha)\n",
    "#         elif epoch > 70 and epoch <= 100 and (epoch + 1) % 5 == 0:\n",
    "#             alpha *= 0.925\n",
    "#             update_learn_rate(optimizer, alpha)\n",
    "#         elif epoch > 100 and (epoch + 1) % 5 == 0:\n",
    "#             alpha *= 0.5\n",
    "#             update_learn_rate(optimizer, alpha)\n",
    "\n",
    "#         pred = net(img)\n",
    "#         loss = criterion(pred, target)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # print statistics\n",
    "#         running_loss += loss.item()\n",
    "#         if i % print_every == print_every - 1:\n",
    "#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n",
    "#             running_loss = 0.0\n",
    "#     scheduler.step()\n",
    "# print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network optimizer\n",
    "optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n",
    "\n",
    "# Warm-up设置\n",
    "warmup_epochs = 0.1*num_epochs  # 设定warm-up的轮数\n",
    "initial_lr = 3e-4  # 初始学习率\n",
    "max_lr = 3e-3  # 最大学习率\n",
    "\n",
    "# 计算每个epoch的学习率\n",
    "def get_lr(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return initial_lr + (max_lr - initial_lr) * (epoch / warmup_epochs)\n",
    "    return max_lr  # warm-up结束后使用最大学习率\n",
    "\n",
    "# 在最后50%阶段设置cosinelr调度器\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*0.5, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 开启异常检测\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# 训练循环\n",
    "losses = []\n",
    "accuracies = []\n",
    "test_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 更新学习率\n",
    "    if epoch <= num_epochs*0.5:\n",
    "        current_lr = get_lr(epoch)\n",
    "    else:\n",
    "        scheduler.step() \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    learning_rates.append(current_lr)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "\n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # 每个epoch结束后进行一次评估\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "    \n",
    "    # 计算平均损失和准确率\n",
    "    avg_test_loss = test_loss / len(loader[\"test\"])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # 记录结果\n",
    "    losses.append(running_loss / print_every)\n",
    "    accuracies.append(accuracy)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Test Accuracy': accuracies\n",
    "})\n",
    "\n",
    "results_df.to_excel('training_results_Variant4.xlsx', index=False)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 训练损失曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试损失曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试准确率曲线\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_Variant4.jpg')\n",
    "plt.show()\n",
    "\n",
    "# 绘制学习率变化曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), learning_rates, label='Learning Rate', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('lr_schedule_Variant4.png')  # 保存图像\n",
    "plt.show()  # 显示图像\n",
    "\n",
    "# 保存模型\n",
    "torch.save(net.state_dict(), 'Variant4.pth')\n",
    "# 保存模型信息\n",
    "model_info = {\n",
    "    \"Optimizer\": type(optimizer).__name__,\n",
    "    \"Initail lr\": initial_lr,\n",
    "    \"max_lr\": max_lr,\n",
    "    \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n",
    "    \"Network Architecture\": str(net),\n",
    "    \"Data Augmentation\": \"Enhanced\"\n",
    "}\n",
    "\n",
    "model_info_df = pd.DataFrame([model_info])\n",
    "model_info_df.to_excel('model_info_variant4.xlsx', index=False)\n",
    "\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        # make prediction\n",
    "        pred = net(img)\n",
    "        \n",
    "        # accumulate\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrain and Evaluate model on MINIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理和加载\n",
    "transformation = dict()\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    if is_train:\n",
    "        # 训练集使用增强的数据增强\n",
    "        transformation[data_type] = tv_transforms.Compose([\n",
    "            # 随机裁剪和填充\n",
    "            tv_transforms.RandomCrop(28, padding=4, padding_mode='reflect'),\n",
    "            # 随机旋转\n",
    "            tv_transforms.RandomRotation(degrees=15),\n",
    "            # 随机仿射变换\n",
    "            tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            # 转换为张量\n",
    "            tv_transforms.ToTensor(),\n",
    "            # 标准化\n",
    "            tv_transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            # 随机擦除\n",
    "            tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n",
    "        ])\n",
    "    else:\n",
    "        # 测试集只进行基本转换\n",
    "        transformation[data_type] = tv_transforms.Compose([\n",
    "            tv_transforms.ToTensor(),\n",
    "            tv_transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        \n",
    "# 加载MNIST数据集\n",
    "num_workers = 2\n",
    "batch_size = 64\n",
    "dataset, loader = {}, {}\n",
    "for data_type in (\"train\", \"test\"):\n",
    "    is_train = data_type==\"train\"\n",
    "    dataset[data_type] = tv_datasets.MNIST(\n",
    "        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n",
    "    )\n",
    "    loader[data_type] = torch.utils.data.DataLoader(\n",
    "        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "# 修改网络架构（仅修改第一层的输入通道）\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "    nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n",
    "    nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n",
    "    nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Linear(1024 * 3 * 3, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048), # 输入：28×28 经过3次MaxPool2d(2)：28 → 14 → 7 → 3 最后特征图大小是3×3\n",
    "    nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n",
    "    nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n",
    "    nn.Linear(512, 10),\n",
    ")\n",
    "\n",
    "# 移动到设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "# 打印参数数量\n",
    "print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n",
    "\n",
    "# 优化器设置\n",
    "optimizer = optim.AdamW(net.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "# Warm-up设置\n",
    "num_epochs = 64\n",
    "warmup_epochs = int(0.1 * num_epochs)\n",
    "initial_lr = 3e-4\n",
    "max_lr = 3e-3\n",
    "\n",
    "# 学习率调整函数\n",
    "def get_lr(epoch):\n",
    "    if epoch < warmup_epochs:\n",
    "        return initial_lr + (max_lr - initial_lr) * (epoch / warmup_epochs)\n",
    "    return max_lr\n",
    "\n",
    "# 余弦退火调度器\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(num_epochs*0.5), eta_min=1e-6)\n",
    "\n",
    "# 损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "losses = []\n",
    "accuracies = []\n",
    "test_losses = []\n",
    "learning_rates = []\n",
    "print_every = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 更新学习率\n",
    "    if epoch <= num_epochs*0.5:\n",
    "        current_lr = get_lr(epoch)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    learning_rates.append(current_lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "\n",
    "    # 训练阶段\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (img, target) in enumerate(loader[\"train\"]):\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        \n",
    "        pred = net(img)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == print_every - 1:\n",
    "            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # 评估阶段\n",
    "    net.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, target in loader[\"test\"]:\n",
    "            img, target = img.to(device), target.to(device)\n",
    "            \n",
    "            pred = net(img)\n",
    "            loss = criterion(pred, target)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            total += len(target)\n",
    "            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(loader[\"test\"])\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    losses.append(running_loss / print_every)\n",
    "    accuracies.append(accuracy)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")\n",
    "\n",
    "# 绘制结果\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 保存训练结果\n",
    "results_df = pd.DataFrame({\n",
    "    'Epoch': range(1, num_epochs + 1),\n",
    "    'Training Loss': losses,\n",
    "    'Test Loss': test_losses,\n",
    "    'Test Accuracy': accuracies\n",
    "})\n",
    "\n",
    "results_df.to_excel('training_results_MNIST.xlsx', index=False)\n",
    "\n",
    "# 可视化训练过程\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# 训练损失曲线\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试损失曲线\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 测试准确率曲线\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_MNIST.jpg')\n",
    "plt.show()\n",
    "\n",
    "# 绘制学习率变化曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), learning_rates, label='Learning Rate', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('Learning Rate Schedule')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig('lr_schedule_MNIST.png')\n",
    "plt.show()\n",
    "\n",
    "# 最终评估\n",
    "net.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for img, target in loader[\"test\"]:\n",
    "        img, target = img.to(device), target.to(device)\n",
    "        pred = net(img)\n",
    "        total += len(target)\n",
    "        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n",
    "\n",
    "print(f\"Final accuracy on MNIST test set: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
