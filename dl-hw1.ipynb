{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 1 - Code Example - Part A\n\nThis code baseline is inspired by and modified from [this great tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n\nThis code can achieve an accuracy of approximately 86.50% on CIFAR-10. Please set up the environment and run your experiments starting from this baseline. You are expected to achieve an accuracy higher than this baseline.","metadata":{}},{"cell_type":"code","source":"# import some necessary packages\n#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchvision.datasets as tv_datasets\nimport torchvision.transforms as tv_transforms\nimport matplotlib.pyplot as plt","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2025-03-12T11:38:13.830212Z","iopub.status.busy":"2025-03-12T11:38:13.829899Z","iopub.status.idle":"2025-03-12T11:38:19.179381Z","shell.execute_reply":"2025-03-12T11:38:19.178542Z","shell.execute_reply.started":"2025-03-12T11:38:13.830187Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    print(\"CUDA Version:\", torch.version.cuda)\n    print(\"CUDA Device:\", torch.cuda.get_device_name(0))\nelse:\n    print(\"CUDA is not available.\")\n\nprint(f\"PyTorch Version: {torch.__version__}\")\nimport torchvision\nprint(f\"torchvision Version: {torchvision.__version__}\")","metadata":{"execution":{"iopub.execute_input":"2025-03-12T11:38:19.180673Z","iopub.status.busy":"2025-03-12T11:38:19.180318Z","iopub.status.idle":"2025-03-12T11:38:19.254308Z","shell.execute_reply":"2025-03-12T11:38:19.253703Z","shell.execute_reply.started":"2025-03-12T11:38:19.180649Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 1. Baseline Model\n### 1.1 Settings","metadata":{}},{"cell_type":"code","source":"# # some experimental setup\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# num_epochs = 128\n# batch_size = 64\n# print_every = 200\n\n# optim_name = \"Adam\"\n# optim_kwargs = dict(\n#     lr=3e-4,\n#     weight_decay=1e-6,\n# )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # prepare datasets\n# # preprocessing pipeline for input images\n# transformation = dict()\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     transformation[data_type] = tv_transforms.Compose(([\n#         tv_transforms.RandomRotation(degrees=15),\n#         tv_transforms.RandomHorizontalFlip(),\n#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n#     ] if is_train else []) + \n#     [\n#         tv_transforms.ToTensor(),\n#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#     ])\n    \n# num_workers = 2\n# dataset, loader = {}, {}\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     dataset[data_type] = tv_datasets.CIFAR10(\n#         root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n#     )\n#     loader[data_type] = torch.utils.data.DataLoader(\n#         dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n#     )\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # our network architecture\n# net = nn.Sequential(\n#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n#     nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n#     nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n#     nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.Dropout(0.3),\n#     nn.Flatten(),\n#     nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.5),\n#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.5),\n#     nn.Linear(128, 10),\n# )\n\n# # move to device\n# net.to(device)\n\n# # print the number of parameters\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.2 Start Training","metadata":{}},{"cell_type":"code","source":"# # the network optimizer\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # training loop\n# net.train()\n# for epoch in range(num_epochs):\n\n#     running_loss = 0.0\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n\n#         pred = net(img)\n#         loss = criterion(pred, target)\n\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         # print statistics\n#         running_loss += loss.item()\n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n\n# print(\"Finished Training\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.3 Evaluating its accuracy","metadata":{}},{"cell_type":"code","source":"# net.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for img, target in loader[\"test\"]:\n#         img, target = img.to(device), target.to(device)\n        \n#         # make prediction\n#         pred = net(img)\n        \n#         # accumulate\n#         total += len(target)\n#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\n# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Variant 1\nAdding batchnorm dropping the dropout layers in conv blocks, and decreasing the dropout ratio in linear layers.  \nTime consuming: 50m   \nlast loss: 0.033   \nTest acc: 89.31%","metadata":{}},{"cell_type":"code","source":"# # some experimental setup\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# num_epochs = 128\n# batch_size = 64\n# print_every = 200\n\n# optim_name = \"Adam\"\n# optim_kwargs = dict(\n#     lr=3e-4,\n#     weight_decay=1e-6,\n# )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # prepare datasets\n# # preprocessing pipeline for input images\n# transformation = dict()\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     transformation[data_type] = tv_transforms.Compose(([\n#         tv_transforms.RandomRotation(degrees=15),\n#         tv_transforms.RandomHorizontalFlip(),\n#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n#     ] if is_train else []) + \n#     [\n#         tv_transforms.ToTensor(),\n#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#     ])\n    \n# num_workers = 2\n# dataset, loader = {}, {}\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     dataset[data_type] = tv_datasets.CIFAR10(\n#         root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n#     )\n#     loader[data_type] = torch.utils.data.DataLoader(\n#         dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n#     )\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # our network architecture\n# net = nn.Sequential(\n#     nn.Conv2d(3, 128, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),nn.BatchNorm2d(128),\n#     nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),nn.BatchNorm2d(256),\n#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),nn.BatchNorm2d(512),\n#     nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),nn.BatchNorm2d(512),\n#     nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2),nn.BatchNorm2d(256),\n#     nn.Flatten(),\n#     nn.Linear(256 * 4 * 4, 512), nn.ReLU(inplace=True), nn.Dropout(0.3),nn.BatchNorm1d(512),\n#     nn.Linear(512, 256), nn.ReLU(inplace=True), nn.Dropout(0.3),nn.BatchNorm1d(256),\n#     nn.Linear(256, 128), nn.ReLU(inplace=True), nn.Dropout(0.3),nn.BatchNorm1d(128),\n#     nn.Linear(128, 10),\n# )\n\n# # move to device\n# net.to(device)\n\n# # print the number of parameters\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the network optimizer\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # 训练循环\n# losses = []\n# accuracies = []\n# test_losses = []\n\n# for epoch in range(num_epochs):\n#     # 训练阶段\n#     net.train()\n#     running_loss = 0.0\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n        \n#         pred = net(img)\n#         loss = criterion(pred, target)\n        \n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n        \n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n    \n#     # 每个epoch结束后进行一次评估\n#     net.eval()\n#     test_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     with torch.no_grad():\n#         for img, target in loader[\"test\"]:\n#             img, target = img.to(device), target.to(device)\n            \n#             pred = net(img)\n#             loss = criterion(pred, target)\n            \n#             test_loss += loss.item()\n#             total += len(target)\n#             correct += (torch.argmax(pred, dim=1) == target).sum().item()\n    \n#     # 计算平均损失和准确率\n#     avg_test_loss = test_loss / len(loader[\"test\"])\n#     accuracy = correct / total\n    \n#     # 记录结果\n#     losses.append(running_loss / print_every)\n#     accuracies.append(accuracy)\n#     test_losses.append(avg_test_loss)\n    \n#     print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n\n# print(\"Finished Training\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# results_df = pd.DataFrame({\n#     'Epoch': range(1, num_epochs + 1),\n#     'Training Loss': losses,\n#     'Test Loss': test_losses,\n#     'Test Accuracy': accuracies\n# })\n\n# results_df.to_excel('training_results_Variant1.xlsx', index=False)\n\n# # 可视化\n# plt.figure(figsize=(18, 5))\n\n# # 训练损失曲线\n# plt.subplot(1, 3, 1)\n# plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Training Loss')\n# plt.legend()\n\n# # 测试损失曲线\n# plt.subplot(1, 3, 2)\n# plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Test Loss')\n# plt.legend()\n\n# # 测试准确率曲线\n# plt.subplot(1, 3, 3)\n# plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.title('Test Accuracy')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.savefig('training_results_Variant1.jpg')\n# plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # save model\n# torch.save(net.state_dict(), 'Variant1_3.pth')\n# # save model info\n# model_info = {\n#     \"Optimizer\": type(optimizer).__name__,\n#     \"Learning Rate\": optimizer.param_groups[0]['lr'],\n#     \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n#     \"Network Architecture\": str(net)\n# }\n\n# model_info_df = pd.DataFrame([model_info])\n# model_info_df.to_excel('model_info_variant1.xlsx', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # evaluation\n# net.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for img, target in loader[\"test\"]:\n#         img, target = img.to(device), target.to(device)\n        \n#         # make prediction\n#         pred = net(img)\n        \n#         # accumulate\n#         total += len(target)\n#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\n# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Variant 2  \nAdd 2 more Conv layers and use wider nets.  \nTime: 70min   \nlast loss: 0.042  \nTest acc: 89.60%","metadata":{}},{"cell_type":"code","source":"# # some experimental setup\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# num_epochs = 64\n# batch_size = 64\n# print_every = 200\n\n# optim_name = \"Adam\"\n# optim_kwargs = dict(\n#     lr=3e-4,\n#     weight_decay=1e-6,\n# )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # prepare datasets\n# # preprocessing pipeline for input images\n# transformation = dict()\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     transformation[data_type] = tv_transforms.Compose(([\n#         tv_transforms.RandomRotation(degrees=15),\n#         tv_transforms.RandomHorizontalFlip(),\n#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n#     ] if is_train else []) + \n#     [\n#         tv_transforms.ToTensor(),\n#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#     ])\n    \n# num_workers = 2\n# dataset, loader = {}, {}\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     dataset[data_type] = tv_datasets.CIFAR10(\n#         root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n#     )\n#     loader[data_type] = torch.utils.data.DataLoader(\n#         dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n#     )\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # our network architecture\n# net = nn.Sequential(\n#     nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n#     nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n#     nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n#     nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n#     nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n#     nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n    \n#     nn.Flatten(),\n    \n#     nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n#     nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n#     nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n#     nn.Linear(512, 10),\n# )\n\n# # move to device\n# net.to(device)\n\n# # print the number of parameters\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the network optimizer\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # 训练循环\n# losses = []\n# accuracies = []\n# test_losses = []\n\n# for epoch in range(num_epochs):\n#     # 训练阶段\n#     net.train()\n#     running_loss = 0.0\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n        \n#         pred = net(img)\n#         loss = criterion(pred, target)\n        \n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n        \n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n    \n#     # 每个epoch结束后进行一次评估\n#     net.eval()\n#     test_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     with torch.no_grad():\n#         for img, target in loader[\"test\"]:\n#             img, target = img.to(device), target.to(device)\n            \n#             pred = net(img)\n#             loss = criterion(pred, target)\n            \n#             test_loss += loss.item()\n#             total += len(target)\n#             correct += (torch.argmax(pred, dim=1) == target).sum().item()\n    \n#     # 计算平均损失和准确率\n#     avg_test_loss = test_loss / len(loader[\"test\"])\n#     accuracy = correct / total\n    \n#     # 记录结果\n#     losses.append(running_loss / print_every)\n#     accuracies.append(accuracy)\n#     test_losses.append(avg_test_loss)\n    \n#     print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n\n# print(\"Finished Training\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# results_df = pd.DataFrame({\n#     'Epoch': range(1, num_epochs + 1),\n#     'Training Loss': losses,\n#     'Test Loss': test_losses,\n#     'Test Accuracy': accuracies\n# })\n\n# results_df.to_excel('training_results_Variant2.xlsx', index=False)\n\n# # 可视化\n# plt.figure(figsize=(18, 5))\n\n# # 训练损失曲线\n# plt.subplot(1, 3, 1)\n# plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Training Loss')\n# plt.legend()\n\n# # 测试损失曲线\n# plt.subplot(1, 3, 2)\n# plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Test Loss')\n# plt.legend()\n\n# # 测试准确率曲线\n# plt.subplot(1, 3, 3)\n# plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.title('Test Accuracy')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.savefig('training_results_Variant2.jpg')\n# plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # save model\n# torch.save(net.state_dict(), 'Variant2.pth')\n# # save model info\n# model_info = {\n#     \"Optimizer\": type(optimizer).__name__,\n#     \"Learning Rate\": optimizer.param_groups[0]['lr'],\n#     \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n#     \"Network Architecture\": str(net)\n# }\n\n# model_info_df = pd.DataFrame([model_info])\n# model_info_df.to_excel('model_info_variant2.xlsx', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # evaluation\n# net.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for img, target in loader[\"test\"]:\n#         img, target = img.to(device), target.to(device)\n        \n#         # make prediction\n#         pred = net(img)\n        \n#         # accumulate\n#         total += len(target)\n#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\n# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Variant 3\nAdding residual connections to be a ResNet, which allows the network to be deeper while avoiding the problems of vanishing or exploding gradients.  \nResNet34:  \nTime consuming: 1h13m  \nlast loss: 0.129  \nTest acc: 84.33%","metadata":{}},{"cell_type":"code","source":"# # some experimental setup\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# num_epochs = 64\n# batch_size = 64\n# print_every = 200\n\n# optim_name = \"Adam\"\n# optim_kwargs = dict(\n#     lr=3e-4,\n#     weight_decay=1e-6,\n# )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # prepare datasets\n# # preprocessing pipeline for input images\n# transformation = dict()\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     transformation[data_type] = tv_transforms.Compose(([\n#         tv_transforms.RandomRotation(degrees=15),\n#         tv_transforms.RandomHorizontalFlip(),\n#         tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n#     ] if is_train else []) + \n#     [\n#         tv_transforms.ToTensor(),\n#         tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#     ])\n    \n# num_workers = 2\n# dataset, loader = {}, {}\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     dataset[data_type] = tv_datasets.CIFAR10(\n#         root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n#     )\n#     loader[data_type] = torch.utils.data.DataLoader(\n#         dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n#     )\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ResNet 34\n# # 定义基本的残差块\n# class ResBlock(nn.Module):\n#     expansion = 1\n\n#     def __init__(self, in_channels, out_channels, stride=1):\n#         super(ResBlock, self).__init__()\n#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n#         self.bn1 = nn.BatchNorm2d(out_channels)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n#         self.bn2 = nn.BatchNorm2d(out_channels)\n\n#         self.shortcut = nn.Sequential()\n#         if stride != 1 or in_channels != self.expansion * out_channels:\n#             self.shortcut = nn.Sequential(\n#                 nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n#                 nn.BatchNorm2d(self.expansion * out_channels)\n#             )\n\n#     def forward(self, x):\n#         out = self.relu(self.bn1(self.conv1(x)))\n#         out = self.bn2(self.conv2(out))\n#         out += self.shortcut(x)\n#         out = self.relu(out)\n#         return out\n\n# # 定义ResNet34模型\n# class ResNet34(nn.Module):\n#     def __init__(self, num_classes=1000):\n#         super(ResNet34, self).__init__()\n#         self.in_channels = 64\n\n#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n#         self.bn1 = nn.BatchNorm2d(64)\n#         self.relu = nn.ReLU(inplace=True)\n#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n#         self.layer1 = self._make_layer(64, 3, stride=1)\n#         self.layer2 = self._make_layer(128, 4, stride=2)\n#         self.layer3 = self._make_layer(256, 6, stride=2)\n#         self.layer4 = self._make_layer(512, 3, stride=2)\n\n#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n#         self.fc = nn.Linear(512 * ResBlock.expansion, num_classes)\n\n#     def _make_layer(self, out_channels, num_blocks, stride):\n#         strides = [stride] + [1] * (num_blocks - 1)\n#         layers = []\n#         for stride in strides:\n#             layers.append(ResBlock(self.in_channels, out_channels, stride))\n#             self.in_channels = out_channels * ResBlock.expansion\n#         return nn.Sequential(*layers)\n\n#     def forward(self, x):\n#         out = self.relu(self.bn1(self.conv1(x)))\n#         out = self.maxpool(out)\n#         out = self.layer1(out)\n#         out = self.layer2(out)\n#         out = self.layer3(out)\n#         out = self.layer4(out)\n#         out = self.avgpool(out)\n#         out = torch.flatten(out, 1)\n#         out = self.fc(out)\n#         return out\n\n\n# net = ResNet34(num_classes=10)\n\n# # move to device\n# net.to(device)\n\n# # print the number of parameters\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 定义基本的残差块\n# class ResBlock(nn.Module):\n#     def __init__(self, in_channels, out_channels, stride=1):\n#         super(ResBlock, self).__init__()\n#         # 主路径\n#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n#         self.bn1 = nn.BatchNorm2d(out_channels)\n#         self.relu = nn.ReLU(inplace=False)\n        \n#         # 残差路径\n#         self.shortcut = nn.Sequential()\n#         if stride != 1 or in_channels != out_channels:\n#             self.shortcut = nn.Sequential(\n#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n#                 nn.BatchNorm2d(out_channels)\n#             )\n\n#     def forward(self, x):\n#         identity = x\n        \n#         out = self.bn1(self.conv1(x))\n#         out = self.relu(out)\n        \n#         # 添加残差连接\n#         out = out + self.shortcut(identity)\n#         out = self.relu(out)\n        \n#         return out\n\n# # 网络结构\n# net = nn.Sequential(\n#     # 第一个卷积块 - 保持与Variant2相同的起始通道数\n#     nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(256),\n#     ResBlock(256, 256),  # 添加残差连接\n#     nn.MaxPool2d(2),\n    \n#     # 第二个卷积块 - 基于Variant2的通道数\n#     ResBlock(256, 512),\n#     ResBlock(512, 512),\n#     nn.MaxPool2d(2),\n    \n#     # 第三个卷积块\n#     ResBlock(512, 1024),\n#     ResBlock(1024, 1024),\n    \n#     # 第四个卷积块\n#     ResBlock(1024, 2048),\n#     ResBlock(2048, 2048),\n    \n#     # 第五个卷积块\n#     ResBlock(2048, 1024),\n#     nn.MaxPool2d(2),\n    \n#     nn.Flatten(),\n    \n#     # 全连接层 - 与Variant2保持一致\n#     nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n#     nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n#     nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n#     nn.Linear(512, 10),\n# )\n\n# # move to device\n# net.to(device)\n\n# # print the number of parameters\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the network optimizer\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # 开启异常检测\n# torch.autograd.set_detect_anomaly(True)\n\n# # 训练循环\n# losses = []\n# accuracies = []\n# test_losses = []\n\n# for epoch in range(num_epochs):\n#     # 训练阶段\n#     net.train()\n#     running_loss = 0.0\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n        \n#         pred = net(img)\n#         loss = criterion(pred, target)\n        \n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n        \n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n    \n#     # 每个epoch结束后进行一次评估\n#     net.eval()\n#     test_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     with torch.no_grad():\n#         for img, target in loader[\"test\"]:\n#             img, target = img.to(device), target.to(device)\n            \n#             pred = net(img)\n#             loss = criterion(pred, target)\n            \n#             test_loss += loss.item()\n#             total += len(target)\n#             correct += (torch.argmax(pred, dim=1) == target).sum().item()\n    \n#     # 计算平均损失和准确率\n#     avg_test_loss = test_loss / len(loader[\"test\"])\n#     accuracy = correct / total\n    \n#     # 记录结果\n#     losses.append(running_loss / print_every)\n#     accuracies.append(accuracy)\n#     test_losses.append(avg_test_loss)\n    \n#     print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n\n# print(\"Finished Training\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# results_df = pd.DataFrame({\n#     'Epoch': range(1, num_epochs + 1),\n#     'Training Loss': losses,\n#     'Test Loss': test_losses,\n#     'Test Accuracy': accuracies\n# })\n\n# results_df.to_excel('training_results_Variant3.xlsx', index=False)\n\n# # 可视化\n# plt.figure(figsize=(18, 5))\n\n# # 训练损失曲线\n# plt.subplot(1, 3, 1)\n# plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Training Loss')\n# plt.legend()\n\n# # 测试损失曲线\n# plt.subplot(1, 3, 2)\n# plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Test Loss')\n# plt.legend()\n\n# # 测试准确率曲线\n# plt.subplot(1, 3, 3)\n# plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.title('Test Accuracy')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.savefig('training_results_Variant3.jpg')\n# plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # save model\n# torch.save(net.state_dict(), 'Variant3.pth')\n# # save model info\n# model_info = {\n#     \"Optimizer\": type(optimizer).__name__,\n#     \"Learning Rate\": optimizer.param_groups[0]['lr'],\n#     \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n#     \"Network Architecture\": str(net)\n# }\n\n# model_info_df = pd.DataFrame([model_info])\n# model_info_df.to_excel('model_info_variant3.xlsx', index=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# net.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for img, target in loader[\"test\"]:\n#         img, target = img.to(device), target.to(device)\n        \n#         # make prediction\n#         pred = net(img)\n        \n#         # accumulate\n#         total += len(target)\n#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\n# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Variant 4\nChanging the data augementation.  \nRef: https://zhuanlan.zhihu.com/p/49180361","metadata":{}},{"cell_type":"code","source":"# # prepare datasets\n# # preprocessing pipeline for input images\n# # 数据增强\n# transformation = dict()\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     if is_train:\n#         # 训练集使用增强的数据增强\n#         transformation[data_type] = tv_transforms.Compose([\n#             # 随机裁剪和填充\n#             tv_transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n#             # 随机水平翻转\n#             tv_transforms.RandomHorizontalFlip(),\n#             # 随机旋转\n#             tv_transforms.RandomRotation(degrees=15),\n#             # 随机仿射变换\n#             tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n#             # 颜色抖动\n#             tv_transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n#             # 转换为张量\n#             tv_transforms.ToTensor(),\n#             # 标准化\n#             tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#             # 随机擦除\n#             tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n#         ])\n#     else:\n#         # 测试集只进行基本转换\n#         transformation[data_type] = tv_transforms.Compose([\n#             tv_transforms.ToTensor(),\n#             tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#         ])\n        \n# num_workers = 2\n# batch_size = 64\n# dataset, loader = {}, {}\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     dataset[data_type] = tv_datasets.CIFAR10(\n#         root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n#     )\n#     loader[data_type] = torch.utils.data.DataLoader(\n#         dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n#     )\n","metadata":{"execution":{"iopub.execute_input":"2025-03-12T11:38:43.674123Z","iopub.status.busy":"2025-03-12T11:38:43.673850Z","iopub.status.idle":"2025-03-12T11:38:50.824034Z","shell.execute_reply":"2025-03-12T11:38:50.823331Z","shell.execute_reply.started":"2025-03-12T11:38:43.674103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # some experimental setup\n# from torch.optim import lr_scheduler\n\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# num_epochs = 64\n# print_every = 200\n\n# optim_name = \"AdamW\"\n# optim_kwargs = dict(\n#     lr=3e-4,\n#     weight_decay=1e-6,\n# )","metadata":{"execution":{"iopub.execute_input":"2025-03-12T11:39:20.117524Z","iopub.status.busy":"2025-03-12T11:39:20.117185Z","iopub.status.idle":"2025-03-12T11:39:20.121664Z","shell.execute_reply":"2025-03-12T11:39:20.120935Z","shell.execute_reply.started":"2025-03-12T11:39:20.117450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 使用Variant2的网络架构\n# net = nn.Sequential(\n#     nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n#     nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n#     nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n#     nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n#     nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n#     nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n    \n#     nn.Flatten(),\n    \n#     nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n#     nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n#     nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n#     nn.Linear(512, 10),\n# )\n# # 移动到设备\n# net.to(device)\n\n# # 打印参数数量\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n\n","metadata":{"collapsed":false,"execution":{"iopub.execute_input":"2025-03-12T11:39:26.918169Z","iopub.status.busy":"2025-03-12T11:39:26.917888Z","iopub.status.idle":"2025-03-12T11:39:28.313816Z","shell.execute_reply":"2025-03-12T11:39:28.313084Z","shell.execute_reply.started":"2025-03-12T11:39:26.918149Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the network optimizer\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # 开启异常检测\n# torch.autograd.set_detect_anomaly(True)\n\n# # 训练循环\n# losses = []\n# accuracies = []\n# test_losses = []\n\n# for epoch in range(num_epochs):\n#     # 训练阶段\n#     net.train()\n#     running_loss = 0.0\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n        \n#         pred = net(img)\n#         loss = criterion(pred, target)\n        \n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n        \n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n    \n#     # 每个epoch结束后进行一次评估\n#     net.eval()\n#     test_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     with torch.no_grad():\n#         for img, target in loader[\"test\"]:\n#             img, target = img.to(device), target.to(device)\n            \n#             pred = net(img)\n#             loss = criterion(pred, target)\n            \n#             test_loss += loss.item()\n#             total += len(target)\n#             correct += (torch.argmax(pred, dim=1) == target).sum().item()\n    \n#     # 计算平均损失和准确率\n#     avg_test_loss = test_loss / len(loader[\"test\"])\n#     accuracy = correct / total\n    \n#     # 记录结果\n#     losses.append(running_loss / print_every)\n#     accuracies.append(accuracy)\n#     test_losses.append(avg_test_loss)\n    \n#     print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n\n# print(\"Finished Training\")","metadata":{"execution":{"iopub.execute_input":"2025-03-12T11:39:56.493569Z","iopub.status.busy":"2025-03-12T11:39:56.493263Z","iopub.status.idle":"2025-03-12T11:39:56.623213Z","shell.execute_reply":"2025-03-12T11:39:56.621658Z","shell.execute_reply.started":"2025-03-12T11:39:56.493543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 保存结果\n# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# results_df = pd.DataFrame({\n#     'Epoch': range(1, num_epochs + 1),\n#     'Training Loss': losses,\n#     'Test Loss': test_losses,\n#     'Test Accuracy': accuracies\n# })\n\n# results_df.to_excel('training_results_Variant5.xlsx', index=False)\n\n# # 可视化\n# plt.figure(figsize=(18, 5))\n\n# # 训练损失曲线\n# plt.subplot(1, 3, 1)\n# plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Training Loss')\n# plt.legend()\n\n# # 测试损失曲线\n# plt.subplot(1, 3, 2)\n# plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Test Loss')\n# plt.legend()\n\n# # 测试准确率曲线\n# plt.subplot(1, 3, 3)\n# plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.title('Test Accuracy')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.savefig('training_results_Variant5.jpg')\n# plt.show()\n\n# # 保存模型\n# torch.save(net.state_dict(), 'Variant5.pth')\n# # 保存模型信息\n# model_info = {\n#     \"Optimizer\": type(optimizer).__name__,\n#     \"Learning Rate\": optimizer.param_groups[0]['lr'],\n#     \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n#     \"Network Architecture\": str(net),\n#     \"Data Augmentation\": \"Enhanced\"\n# }\n\n# model_info_df = pd.DataFrame([model_info])\n# model_info_df.to_excel('model_info_variant5.xlsx', index=False)\n\n# net.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for img, target in loader[\"test\"]:\n#         img, target = img.to(device), target.to(device)\n        \n#         # make prediction\n#         pred = net(img)\n        \n#         # accumulate\n#         total += len(target)\n#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\n# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Variant 5\nChanging the optimizer and learning rate scheduler.  \nAdd warmup in the first warmup_epochs, then keeping max_lr and cooldown with cosinelr after 50% num_epochs.  ","metadata":{}},{"cell_type":"code","source":"# # prepare datasets\n# # preprocessing pipeline for input images\n# # 数据增强\n# transformation = dict()\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     if is_train:\n#         # 训练集使用增强的数据增强\n#         transformation[data_type] = tv_transforms.Compose([\n#             # 随机裁剪和填充\n#             tv_transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n#             # 随机水平翻转\n#             tv_transforms.RandomHorizontalFlip(),\n#             # 随机旋转\n#             tv_transforms.RandomRotation(degrees=15),\n#             # 随机仿射变换\n#             tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n#             # 颜色抖动\n#             tv_transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n#             # 转换为张量\n#             tv_transforms.ToTensor(),\n#             # 标准化\n#             tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#             # 随机擦除\n#             tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n#         ])\n#     else:\n#         # 测试集只进行基本转换\n#         transformation[data_type] = tv_transforms.Compose([\n#             tv_transforms.ToTensor(),\n#             tv_transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n#         ])\n        \n# num_workers = 2\n# batch_size = 64\n# dataset, loader = {}, {}\n# for data_type in (\"train\", \"test\"):\n#     is_train = data_type==\"train\"\n#     dataset[data_type] = tv_datasets.CIFAR10(\n#         root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n#     )\n#     loader[data_type] = torch.utils.data.DataLoader(\n#         dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n#     )\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # some experimental setup\n# from torch.optim import lr_scheduler\n\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# num_epochs = 64\n# print_every = 200\n\n# optim_name = \"AdamW\"\n# optim_kwargs = dict(\n#     lr=3e-4,\n#     weight_decay=1e-6,\n# )","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Results\nAdamW lr=3e-4,weight_decay=0.01 scheduler = lr_scheduler.StepLR(optimizer, step_size=15,gamma=0.1) last_loss = 0.464 acc = 83.82%\n\nAdamW lr=3e-4,weight_decay=0.01\nconstant + cooldown last_loss = 1.015 acc = 68.65%\n","metadata":{}},{"cell_type":"code","source":"# # 使用Variant2的网络架构\n# net = nn.Sequential(\n#     nn.Conv2d(3, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n#     nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n#     nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n#     nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n#     nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n#     nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n#     nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n    \n#     nn.Flatten(),\n    \n#     nn.Linear(1024 * 4 * 4, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048),\n#     nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n#     nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n#     nn.Linear(512, 10),\n# )\n# # 移动到设备\n# net.to(device)\n\n# # 打印参数数量\n# print(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n\n","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the network optimizer\n# 之前的\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # the network lr_scheduler\n# # scheduler = lr_scheduler.StepLR(optimizer, step_size=15,gamma=0.1)\n# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=num_epochs) #或者20？\n# # constant + cooldown\n# def update_learn_rate(optimizer, alpha):\n#     for param_group in optimizer.param_groups:\n#         param_group['lr'] = alpha","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # training loop\n# 这个需要整理一下，如果要放的话\n# net.train()\n# for epoch in range(num_epochs):\n    \n#     running_loss = 0.0\n#     alpha = lr\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n#         # 调整学习率\n#         if epoch <= 30 and (epoch + 1) % 5 == 0:\n#             alpha *= 0.98\n#             update_learn_rate(optimizer, alpha)  # 更新学习率\n#         elif epoch > 30 and epoch <= 70 and (epoch + 1) % 5 == 0:\n#             alpha *= 0.95\n#             update_learn_rate(optimizer, alpha)\n#         elif epoch > 70 and epoch <= 100 and (epoch + 1) % 5 == 0:\n#             alpha *= 0.925\n#             update_learn_rate(optimizer, alpha)\n#         elif epoch > 100 and (epoch + 1) % 5 == 0:\n#             alpha *= 0.5\n#             update_learn_rate(optimizer, alpha)\n\n#         pred = net(img)\n#         loss = criterion(pred, target)\n\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         # print statistics\n#         running_loss += loss.item()\n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n#     scheduler.step()\n# print(\"Finished Training\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # the network optimizer\n# optimizer = getattr(optim, optim_name)(net.parameters(), **optim_kwargs)\n\n# # Warm-up设置\n# warmup_epochs = 0.1*num_epochs  # 设定warm-up的轮数\n# initial_lr = 3e-4  # 初始学习率\n# max_lr = 3e-3  # 最大学习率\n\n# # 计算每个epoch的学习率\n# def get_lr(epoch):\n#     if epoch < warmup_epochs:\n#         return initial_lr + (max_lr - initial_lr) * (epoch / warmup_epochs)\n#     return max_lr  # warm-up结束后使用最大学习率\n\n# # 在最后50%阶段设置cosinelr调度器\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs*0.5, eta_min=1e-6)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # loss function\n# criterion = nn.CrossEntropyLoss()\n\n# # 开启异常检测\n# torch.autograd.set_detect_anomaly(True)\n\n# # 训练循环\n# losses = []\n# accuracies = []\n# test_losses = []\n# learning_rates = []\n\n# for epoch in range(num_epochs):\n#     # 更新学习率\n#     if epoch <= num_epochs*0.5:\n#         current_lr = get_lr(epoch)\n#     else:\n#         scheduler.step() \n#         current_lr = optimizer.param_groups[0]['lr']\n\n#     learning_rates.append(current_lr)\n\n#     for param_group in optimizer.param_groups:\n#         param_group['lr'] = current_lr\n\n#     # 训练阶段\n#     net.train()\n#     running_loss = 0.0\n#     for i, (img, target) in enumerate(loader[\"train\"]):\n#         img, target = img.to(device), target.to(device)\n        \n#         pred = net(img)\n#         loss = criterion(pred, target)\n        \n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n        \n#         running_loss += loss.item()\n        \n#         if i % print_every == print_every - 1:\n#             print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n#             running_loss = 0.0\n    \n#     # 每个epoch结束后进行一次评估\n#     net.eval()\n#     test_loss = 0.0\n#     correct = 0\n#     total = 0\n    \n#     with torch.no_grad():\n#         for img, target in loader[\"test\"]:\n#             img, target = img.to(device), target.to(device)\n            \n#             pred = net(img)\n#             loss = criterion(pred, target)\n            \n#             test_loss += loss.item()\n#             total += len(target)\n#             correct += (torch.argmax(pred, dim=1) == target).sum().item()\n    \n#     # 计算平均损失和准确率\n#     avg_test_loss = test_loss / len(loader[\"test\"])\n#     accuracy = correct / total\n    \n#     # 记录结果\n#     losses.append(running_loss / print_every)\n#     accuracies.append(accuracy)\n#     test_losses.append(avg_test_loss)\n    \n#     print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n\n# print(\"Finished Training\")\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 保存结果\n# import pandas as pd\n# import matplotlib.pyplot as plt\n\n# results_df = pd.DataFrame({\n#     'Epoch': range(1, num_epochs + 1),\n#     'Training Loss': losses,\n#     'Test Loss': test_losses,\n#     'Test Accuracy': accuracies\n# })\n\n# results_df.to_excel('training_results_Variant4.xlsx', index=False)\n\n# # 可视化\n# plt.figure(figsize=(18, 5))\n\n# # 训练损失曲线\n# plt.subplot(1, 3, 1)\n# plt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Training Loss')\n# plt.legend()\n\n# # 测试损失曲线\n# plt.subplot(1, 3, 2)\n# plt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Test Loss')\n# plt.legend()\n\n# # 测试准确率曲线\n# plt.subplot(1, 3, 3)\n# plt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy')\n# plt.title('Test Accuracy')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.savefig('training_results_Variant4.jpg')\n# plt.show()\n\n# # 绘制学习率变化曲线\n# plt.figure(figsize=(10, 5))\n# plt.plot(range(num_epochs), learning_rates, label='Learning Rate', color='blue')\n# plt.xlabel('Epochs')\n# plt.ylabel('Learning Rate')\n# plt.title('Learning Rate Schedule')\n# plt.legend()\n# plt.grid()\n# plt.savefig('lr_schedule_Variant4.png')  # 保存图像\n# plt.show()  # 显示图像\n\n# # 保存模型\n# torch.save(net.state_dict(), 'Variant4.pth')\n# # 保存模型信息\n# model_info = {\n#     \"Optimizer\": type(optimizer).__name__,\n#     \"Initail lr\": initial_lr,\n#     \"max_lr\": max_lr,\n#     \"Weight Decay\": optimizer.param_groups[0]['weight_decay'],\n#     \"Network Architecture\": str(net),\n#     \"Data Augmentation\": \"Enhanced\"\n# }\n\n# model_info_df = pd.DataFrame([model_info])\n# model_info_df.to_excel('model_info_variant4.xlsx', index=False)\n\n# net.eval()\n# correct, total = 0, 0\n# with torch.no_grad():\n#     for img, target in loader[\"test\"]:\n#         img, target = img.to(device), target.to(device)\n        \n#         # make prediction\n#         pred = net(img)\n        \n#         # accumulate\n#         total += len(target)\n#         correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\n# print(f\"Accuracy of the network on the {total} test images: {100 * correct / total:.2f}%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Retrain and Evaluate model on MINIST","metadata":{}},{"cell_type":"code","source":"# 数据预处理和加载\ntransformation = dict()\nfor data_type in (\"train\", \"test\"):\n    is_train = data_type==\"train\"\n    if is_train:\n        # 训练集使用增强的数据增强\n        transformation[data_type] = tv_transforms.Compose([\n            # 随机裁剪和填充\n            tv_transforms.RandomCrop(28, padding=4, padding_mode='reflect'),\n            # 随机旋转\n            tv_transforms.RandomRotation(degrees=15),\n            # 随机仿射变换\n            tv_transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n            # 转换为张量\n            tv_transforms.ToTensor(),\n            # 标准化\n            tv_transforms.Normalize((0.1307,), (0.3081,)),\n            # 随机擦除\n            tv_transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),\n        ])\n    else:\n        # 测试集只进行基本转换\n        transformation[data_type] = tv_transforms.Compose([\n            tv_transforms.ToTensor(),\n            tv_transforms.Normalize((0.1307,), (0.3081,)),\n        ])\n        \n# 加载MNIST数据集\nnum_workers = 2\nbatch_size = 64\ndataset, loader = {}, {}\nfor data_type in (\"train\", \"test\"):\n    is_train = data_type==\"train\"\n    dataset[data_type] = tv_datasets.MNIST(\n        root=\"./data\", train=is_train, download=True, transform=transformation[data_type],\n    )\n    loader[data_type] = torch.utils.data.DataLoader(\n        dataset[data_type], batch_size=batch_size, shuffle=is_train, num_workers=num_workers,\n    )\n\n# 修改网络架构（仅修改第一层的输入通道）\nnet = nn.Sequential(\n    nn.Conv2d(1, 256, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n    nn.Conv2d(512, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n    nn.Conv2d(1024, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(1024),\n    nn.Conv2d(1024, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n    nn.Conv2d(2048, 2048, 3, padding=1), nn.ReLU(inplace=True), nn.BatchNorm2d(2048),\n    nn.Conv2d(2048, 1024, 3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n    \n    nn.Flatten(),\n    \n    nn.Linear(1024 * 3 * 3, 2048), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(2048), # 输入：28×28 经过3次MaxPool2d(2)：28 → 14 → 7 → 3 最后特征图大小是3×3\n    nn.Linear(2048, 1024), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(1024),\n    nn.Linear(1024, 512), nn.ReLU(inplace=True), nn.Dropout(0.3), nn.BatchNorm1d(512),\n    nn.Linear(512, 10),\n)\n\n# 移动到设备\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnet.to(device)\n\n# 打印参数数量\nprint(f\"number of parameters: {sum(p.numel() for p in net.parameters() if p.requires_grad) / 1_000_000:.2f}M\")\n\n# 优化器设置\noptimizer = optim.AdamW(net.parameters(), lr=3e-4, weight_decay=1e-6)\n\n# Warm-up设置\nnum_epochs = 64\nwarmup_epochs = int(0.1 * num_epochs)\ninitial_lr = 3e-4\nmax_lr = 3e-3\n\n# 学习率调整函数\ndef get_lr(epoch):\n    if epoch < warmup_epochs:\n        return initial_lr + (max_lr - initial_lr) * (epoch / warmup_epochs)\n    return max_lr\n\n# 余弦退火调度器\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(num_epochs*0.5), eta_min=1e-6)\n\n# 损失函数\ncriterion = nn.CrossEntropyLoss()\n\n# 训练循环\nlosses = []\naccuracies = []\ntest_losses = []\nlearning_rates = []\nprint_every = 100\n\nfor epoch in range(num_epochs):\n    # 更新学习率\n    if epoch <= num_epochs*0.5:\n        current_lr = get_lr(epoch)\n    else:\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n    \n    learning_rates.append(current_lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = current_lr\n\n    # 训练阶段\n    net.train()\n    running_loss = 0.0\n    for i, (img, target) in enumerate(loader[\"train\"]):\n        img, target = img.to(device), target.to(device)\n        \n        pred = net(img)\n        loss = criterion(pred, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        if i % print_every == print_every - 1:\n            print(f\"[epoch={epoch + 1:3d}, iter={i + 1:5d}] training loss: {running_loss / print_every:.3f}\")\n            running_loss = 0.0\n    \n    # 评估阶段\n    net.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for img, target in loader[\"test\"]:\n            img, target = img.to(device), target.to(device)\n            \n            pred = net(img)\n            loss = criterion(pred, target)\n            \n            test_loss += loss.item()\n            total += len(target)\n            correct += (torch.argmax(pred, dim=1) == target).sum().item()\n    \n    avg_test_loss = test_loss / len(loader[\"test\"])\n    accuracy = correct / total\n    \n    losses.append(running_loss / print_every)\n    accuracies.append(accuracy)\n    test_losses.append(avg_test_loss)\n    \n    print(f\"Epoch {epoch + 1}: Test Loss: {avg_test_loss:.3f}, Accuracy: {100 * accuracy:.2f}%\")\n\nprint(\"Finished Training\")\n\n# 绘制结果\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 保存训练结果\nresults_df = pd.DataFrame({\n    'Epoch': range(1, num_epochs + 1),\n    'Training Loss': losses,\n    'Test Loss': test_losses,\n    'Test Accuracy': accuracies\n})\n\nresults_df.to_excel('training_results_MNIST.xlsx', index=False)\n\n# 可视化训练过程\nplt.figure(figsize=(18, 5))\n\n# 训练损失曲线\nplt.subplot(1, 3, 1)\nplt.plot(results_df['Epoch'], results_df['Training Loss'], label='Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend()\n\n# 测试损失曲线\nplt.subplot(1, 3, 2)\nplt.plot(results_df['Epoch'], results_df['Test Loss'], label='Test Loss', color='orange')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Test Loss')\nplt.legend()\n\n# 测试准确率曲线\nplt.subplot(1, 3, 3)\nplt.plot(results_df['Epoch'], results_df['Test Accuracy'], label='Test Accuracy', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Test Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.savefig('training_results_MNIST.jpg')\nplt.show()\n\n# 绘制学习率变化曲线\nplt.figure(figsize=(10, 5))\nplt.plot(range(num_epochs), learning_rates, label='Learning Rate', color='blue')\nplt.xlabel('Epochs')\nplt.ylabel('Learning Rate')\nplt.title('Learning Rate Schedule')\nplt.legend()\nplt.grid()\nplt.savefig('lr_schedule_MNIST.png')\nplt.show()\n\n# 最终评估\nnet.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for img, target in loader[\"test\"]:\n        img, target = img.to(device), target.to(device)\n        pred = net(img)\n        total += len(target)\n        correct += (torch.argmax(pred, dim=1) == target).sum().item()\n\nprint(f\"Final accuracy on MNIST test set: {100 * correct / total:.2f}%\")","metadata":{},"outputs":[],"execution_count":null}]}